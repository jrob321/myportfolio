install.packages('MASS')
library('MASS')
#Part 1
#a
#H0: X and Y are independent Ha: H0 is false
#b
#The expected count is found by adding up the values in the ith row, which the (i,j) cell is
located
#and multiplying it by the sum of the jth column. You then divide that by the sum of all cells in
#order to get the expected count for (i,j).
#c
#The test statistic of the chi square test is found with the formula Σ (Oi-Ei)^2/Ei
#Where Σ is the summation symbol, Oi is the observed value of a specific cell, and Ei is the
expected
#value of a specific cell. The null distribution is the probability that the test statistic
#is obtained when the null hypothesis is true.
#d
#A large test statistic provides evidence against the null because it means that the p-value is
#likely to be smaller. A smaller p-value means that the null hypothesis is rejected in more
instances.
#e
#P-value is P(X<x) where x is the test statistic
#f
chi_square <- function(dat, res.type="pearson"){
rows <- matrix(rowSums(dat))
stop <- nrow(dat)
while (stop>1){
rows <- cbind(rows,rowSums(dat))
stop <- stop-1
}
cols <- colSums(dat)
stop <- ncol(dat)
while (stop>1){
cols <- rbind(cols,colSums(dat))
stop <- stop-1
}
e <- (cols * rows)/sum(dat)
t_stat <- sum((dat-e)^2/e)
p_value <- pchisq(t_stat,df=length(dat)-1,lower.tail = FALSE)
if (res.type=="pearson"){
residuals <- (dat-e)/sqrt(e)
}
if (res.type=="std"){
residuals <- (dat-e)/sqrt(e*(1-(dat/rows))*(1-(dat/cols)))
}
for (n in e){
if (n < 5) print("Expected values are less than 5. There might be innacuracy")
}
list(t_stat,p_value,e,res.type,residuals)
}
m1 <- matrix(c(6,115,256,18,256,442,13,136,155),nrow=3,ncol=3)
chi_square(m1)
#The p-value is less than 0.05, so we must reject the null hypothesis. This means that the two
#variables, X and Y, are dependent.
#Part 2
#a
#H0: σ1=σ2 Ha:σ1>σ2 Ha:σ1<σ2 Ha:σ1≠σ2
#b other part
#The test statistic is found by dividing the variance of one sample by the variance of the other.
#The null distribution is the probability that the test statistic is obtained when the null
#hypothesis is true. In this case, it is the area under the curve to the right of the test statistic.
#c
# Ha:σ1≠σ2 is P-value is P(X<x) where x is the probability that the variances are not equal
# Ha:σ1>σ2 is P-value is P(X<x) where x is the probability that variance1 is greater than
variance2
# Ha:σ1<σ2 is P-value is P(X<x) where x is the probability that variance1 is less than variance2
#If the p-value is less than the significance level then the null hypothesis is rejected.
#d
#A confidence interval is constructed by taking the quantile of the significance level at the
#correct degrees of freedom. You then divide 1 by that value and multiply the result by the
#test statistic. The upper bound is found by doing the same thing, but using 1 minus the
#significance level instead of using the significance level.
#e
f_test <- function(y1,y2,alt='two-sided',lev=0.95){
if (shapiro.test(y1)[2] < 0.05){
print("P-value for the Shapiro-Wilk test is less than 0.05. The data may not be normally
distributed")
}
if (shapiro.test(y2)[2] < 0.05){
print("P-value for the Shapiro-Wilk test is less than 0.05. The data may not be normally
distributed")
}
add <- 0
for (n in y1){
add <- add + (n-mean(y1))^2
}
var1 <- add/(length(y1)-1)
add2 <- 0
for (n in y2){
add2 <- add2 + (n-mean(y2))^2
}
var2 <- add2/(length(y2)-1)
if (var1 >= var2){
test_statistic <- var2/var1
}
else{ test_statistic <- var1/var2}
df1 <- length(y1)-1
df2 <- length(y2)-1
if (alt=='two-sided'){
p_value <- pf(test_statistic,length(y1)-1,length(y2)-1)*2
upper_bound <- 1/qf((1-lev)/2,df1,df2)*test_statistic
lower_bound <- 1/qf(1-(1-lev)/2,df1,df2)*test_statistic
}
if (alt=='greater'){
p_value <- 1-pf(test_statistic,length(y1)-1,length(y2)-1)
upper_bound <- Inf
lower_bound <- 1/qf(1-(1-lev),df1,df2)*test_statistic
}
if (alt=='less'){
p_value <- pf(test_statistic,length(y1)-1,length(y2)-1)
upper_bound <- 1/qf((1-lev),df1,df2)*test_statistic
lower_bound <- 0
}
confidence_interval <- paste0("(", lower_bound, ", ", upper_bound, " )")
list(test_statistic, p_value, confidence_interval)
}
#f
c1 <- c(139,131,147,108,122,129,140,144,86,104)
c2 <- c(169,239,120,124,99,113,96,125)
f_test(c1,c2,alt='two-sided')
#The p-value of 0.0196 is greater than the confidence level of 0.01, so we do not reject the null
hypothesis
#The 99% confidence interval is (0.0208,1.219). This means that there is a 99% chance that the
#ratio of the population variances falls in this range.
#Part 3
#a do the other part
#The test statistic is the ratio between the variances of the two samples.
#The degrees of freedom is found by using the Satterthwaite approximation which is equal to
#df=(s1/n1+s2/n2)^2/((1/n1-1)(s1/n1)^2+(1/n2-1)(s2/n2)^2). The null distribution is the
probability
#that the test statistic is obtained when the null hypothesis is true.
#b
#The confidence interval is constructed by taking the quantile of the significance level divided by
#two and with the correct degrees of freedom. You then add the test statistic and multiply by
#the standard error. You then take this value and multiply by -1 to get the lower bound.
#c
t_test <- function(y1,y2,alt='two-sided',lev=0.95){
add <- 0
for (n in y1){
add <- add + (n-mean(y1))^2
}
var1 <- add/(length(y1)-1)
add2 <- 0
for (n in y2){
add2 <- add2 + (n-mean(y2))^2
}
var2 <- add2/(length(y2)-1)
if (var1 >= var2){
test_statistic <- var2/var1
}
else{ test_statistic <- var1/var2}
if (alt=='two-sided'){
fp_value <- pf(test_statistic,length(y1)-1,length(y2)-1)*2
}
if (alt=='greater'){
fp_value <- 1-pf(test_statistic,length(y1)-1,length(y2)-1)
}
if (alt=='less'){
fp_value <- pf(test_statistic,length(y1)-1,length(y2)-1)
}
v1 <- add/(length(y1)-1)
v2 <- add2/(length(y2)-1)
s2 <- ((length(y1)-1)*v1+(length(y2)-1)*v2)/((length(y1)-1)+(length(y2)-1))
if (fp_value > 0.05){
df <- length(y1)+length(y2)-2
v <- (((length(y1)-1)*v1 + ((length(y2)-1)*v2)))/df
stderr <- sqrt(v*(1/length(y1)+1/length(y2)))
test_statistic <- (mean(y1)-mean(y2))/(sqrt(s2)*sqrt((1/length(y1))+1/length(y2)))
test_name <- "two-sample t-test with unknown but equal variances"
}else{
test_statistic <- (mean(y1)-mean(y2))/(sqrt((add2/(length(y2)-1))/length(y2)+(add/
(length(y1)-1))/length(y1)))
test_name <- "two-sample t-test with unknown and unequal variances"
stderr <- sqrt((sqrt(v1/length(y1)))^2 + (sqrt(v2/length(y2)))^2)
df <- stderr^4/((sqrt(v1/length(y1)))^4/(length(y1)-1) + (sqrt(v2/length(y2)))^4/(length(y2)-1))
}
if (alt=="two-sided"){
p_value <- pt(test_statistic,length(y1)+length(y2)-2)*2
lower.bound <- (-qt(1 - (1-lev)/2, df)+test_statistic)*stderr
upper.bound <- (qt(1-(1-lev)/2, df)+test_statistic)*stderr
}
if (alt=="greater"){
p_value <- 1-pt(test_statistic,length(y1)+length(y2)-2)
lower.bound <- (qt((1-lev),df) + test_statistic)*stderr
upper.bound <- Inf
}
if (alt=="less"){
p_value <- pt(test_statistic,length(y1)+length(y2)-2)
lower.bound <- -Inf
upper.bound <- (test_statistic - qt((1-lev),df))*stderr
}
confidence_interval <- paste0("(", lower.bound, ", ", upper.bound, " )")
list(test_name,test_statistic, p_value, confidence_interval)
}
#d
usa <- subset(Cars93,Origin %in% c('USA'))[['MPG.city']]
non_usa <- subset(Cars93,Origin %in% c('non-USA'))[['MPG.city']]
t_test(usa,non_usa)
#The variance test suggests that the two samples don't have equal variances, so the t-test
#is calculated differently than in HW6. The test statistic, p-value, and confidence interval are
#all different than in HW6.
